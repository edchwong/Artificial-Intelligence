from pacai.agents.learning.value import ValueEstimationAgent


class ValueIterationAgent(ValueEstimationAgent):
    """
    A value iteration agent.

    Make sure to read `pacai.agents.learning` before working on this class.

    A `ValueIterationAgent` takes a `pacai.core.mdp.MarkovDecisionProcess` on initialization,
    and runs value iteration for a given number of iterations using the supplied discount factor.

    Some useful mdp methods you will use:
    `pacai.core.mdp.MarkovDecisionProcess.getStates`,
    `pacai.core.mdp.MarkovDecisionProcess.getPossibleActions`,
    `pacai.core.mdp.MarkovDecisionProcess.getTransitionStatesAndProbs`,
    `pacai.core.mdp.MarkovDecisionProcess.getReward`.

    Additional methods to implement:

    `pacai.agents.learning.value.ValueEstimationAgent.getQValue`:
    The q-value of the state action pair (after the indicated number of value iteration passes).
    Note that value iteration does not necessarily create this quantity,
    and you may have to derive it on the fly.

    `pacai.agents.learning.value.ValueEstimationAgent.getPolicy`:
    The policy is the best action in the given state
    according to the values computed by value iteration.
    You may break ties any way you see fit.
    Note that if there are no legal actions, which is the case at the terminal state,
    you should return None.
    """

    def __init__(self, index, mdp, discountRate=0.9, iters=100, **kwargs):
        super().__init__(index, **kwargs)

        self.mdp = mdp
        self.discountRate = discountRate
        self.iters = iters
        self.values = {}  # A dictionary which holds the q-values for each state.

        # Compute the values here.
        # T(s,a,s') = getTransitionStatesAndProbs(self, state, action)
        # Q* (s,a) = Sigma T(s,a,s') [R(s,a,s')+discount* v*(s')]
        # iterates iters # of times
        # finds all legal/possible actions for a given state, and checks the
        # Q value for states generated by such action then updates the values
        # for the next iteration

        allStates = mdp.getStates()
        for i in range(iters):
            stateQValues = self.values.copy()
            for state in allStates:
                possibleActions = mdp.getPossibleActions(state)
                possibleQValues = []
                for action in possibleActions:
                    possibleQValues.append(self.getQValue(state, action))
                if len(possibleActions) != 0:
                    stateQValues[state] = max(possibleQValues)
            self.values = stateQValues

        # raise NotImplementedError()

    # gets all states that can be reached from taking the state taking an action
    # and then totals them up as the Q value for that state
    def getQValue(self, state, action):
        next = self.mdp.getTransitionStatesAndProbs(state, action)
        QValue = 0
        for nextState, prob in next:
            reward = self.mdp.getReward(state, action, nextState)
            QValue += prob * (reward + (self.discountRate * self.getValue(nextState)))

        return QValue

    # finds all legal/possible actions
    # gets the Q value of those states, and then returns the action
    # with the highest Q value
    def getPolicy(self, state):
        possibleActions = self.mdp.getPossibleActions(state)
        if len(possibleActions) == 0:
            return None
        possibleQValues = []
        for action in possibleActions:
            possibleQValues.append(self.getQValue(state, action))
        maxQ = -99999999
        bestActionIndex = 0
        for index, value in enumerate(possibleQValues):
            if value > maxQ:
                maxQ = value
                bestActionIndex = index
        return possibleActions[bestActionIndex]

    def getValue(self, state):
        """
        Return the value of the state (computed in __init__).
        """

        return self.values.get(state, 0.0)

    def getAction(self, state):
        """
        Returns the policy at the state (no exploration).
        """

        return self.getPolicy(state)
